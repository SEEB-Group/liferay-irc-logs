--- Log opened Thu Jul 31 00:00:37 2014
00:52 -!- mode/#liferay [+o rotty3000] by ChanServ
09:12 < jardineworks:#liferay> hey bijoo -- lemme know when you make it into the office
09:15 < bijoo_:#liferay> jardineworks: hey I'm here; debug shutdown correct?
09:26 < jardineworks:#liferay> bijoo_, give me 5 minutes and I'll give you the background on everything
09:27 < jardineworks:#liferay> just need to wrap up a section of a tutorial quick..
09:40 < bijoo_:#liferay> jardineworks: take your time my day's getting swamped right now
09:49 < jardineworks:#liferay> bijoo_, Alright.. a little reading material. My current client wants to have some extended HA Fail over. What we have in an environment where there are two Liferay installations -- serverA and serverB with your typical proxy fronting as a load balancer. serverA and serverB are unicast Liferay clustered so ehcache and lucene are kept in synch. The proxy is using sticky session with a non-JVM route... the proxy basically uses it
09:49 < jardineworks:#liferay> s own cookie. So far all that is good. For added HA though what we have done is clustered (tomcat clustered that is) the two servers leveraging the tribes stuff that comes with the platform. The membership discovery and heart beat is done using a multicast. We've also configure the DetalManager for session replication -- the idea being that if I am half way through a payment process on server A and for some reason it goes down
09:49 < jardineworks:#liferay> , I am bumped to server B but my session is still in tact so I continue along. All of this works -- painful to setup, but it works. Now, if I disable a server in apache (load balancer) using the balancer-manager it works. If I kill -9 the liferay process -- it works. BUT if I shutdown the server using the graceful shutdown process, lets say I shutdown A, then in server B I see a tribes message that comes across with a SESSION-
09:49 < jardineworks:#liferay> EXPIRE message. The delta manager has expireSessionsOnShutdown="false" set which is supposed to prevent this action from happening. At first I thought perhaps it was a bug in tomcat. So I installed and clustered two vanilla tomcat 7.0.42 servers. I tried the same test and the SESSION-EXPIRE was not sent. So at this stage I am thinkiing that it is something in LR. So the question is...IS THERE A LISTENER IN LR THAT IS WIRED TO
09:50 < jardineworks:#liferay> THE _SERVER_ SHUTDOWN PROCESS? and if there is, do you know what it is?
09:50 < jardineworks:#liferay> </end-of-story>
09:50 < jardineworks:#liferay> :)
10:00  * bijoo_:#liferay is reading
10:04 < bijoo_:#liferay> jardineworks: okay; yes, probably there is something. Looking at portal.properties file:
10:05 < bijoo_:#liferay> jardineworks: global.shutdown.events=com.liferay.portal.events.GlobalShutdownAction
10:05 < bijoo_:#liferay> jardineworks: application.shutdown.events=com.liferay.portal.events.AppShutdownAction,com.liferay.portal.events.ChannelHubAppShutdownAction
10:05 < bijoo_:#liferay> jardineworks: shutdown.programmatically.exit=false
10:06 < bijoo_:#liferay> jardineworks: these may not be the only check places; also way to shutdown via UI; good to check what that calls in the backend; and start the debugger with breakpoint there.
10:10 < jardineworks:#liferay> bijoo_, Right -- I saw those classes and had a quick look at them but didn't see anything. I'll take a closer look at them. I have a test portlet called "session-replication-test" and the message contains it as the context (since I guess it is the one with the only active session). I am wondering if during shutdown, the unregister actions trigger somehting in those portlets to cause a session expire BEFORE the server shutsdown
10:10 < jardineworks:#liferay> . Know what I mean?
10:11 < bijoo_:#liferay> jardineworks: yes, so good candidate to debug there.
10:12 < bijoo_:#liferay> jardineworks: it takes time to debug, but I like it; since it gives all information I need.
10:12 < bijoo_:#liferay> jardineworks: have you tried stepping through that process yet?
10:17 < jardineworks:#liferay> bijoo_, not yet but I guess that'll be my next step. other "priorities" have come up though :)
